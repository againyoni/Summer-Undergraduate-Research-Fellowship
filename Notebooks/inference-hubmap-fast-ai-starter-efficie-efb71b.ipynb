{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec7c43b",
   "metadata": {
    "papermill": {
     "duration": 0.008374,
     "end_time": "2022-08-02T08:24:14.074400",
     "exception": false,
     "start_time": "2022-08-02T08:24:14.066026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In particular, we have made inferences with reference to [HuBMAP fast.ai starter (EfficientNet) by wangkui](https://www.kaggle.com/code/befunny/hubmap-fast-ai-starter-efficientnet)\n",
    "\n",
    "## Reference\n",
    "\n",
    "- [HuBMAP fast.ai starter (EfficientNet) by wangkui](https://www.kaggle.com/code/befunny/hubmap-fast-ai-starter-efficientnet)\n",
    "- [Converting to 256x256 by The Devastator](https://www.kaggle.com/code/thedevastator/converting-to-256x256)\n",
    "- [[Training] - FastAI Baseline by The Devastator](https://www.kaggle.com/code/thedevastator/training-fastai-baseline)\n",
    "- [[Inference] - FastAI Baseline by The Devastator](https://www.kaggle.com/code/thedevastator/inference-fastai-baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43794cd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:14.091032Z",
     "iopub.status.busy": "2022-08-02T08:24:14.090293Z",
     "iopub.status.idle": "2022-08-02T08:24:18.487286Z",
     "shell.execute_reply": "2022-08-02T08:24:18.486002Z"
    },
    "papermill": {
     "duration": 4.408749,
     "end_time": "2022-08-02T08:24:18.490427",
     "exception": false,
     "start_time": "2022-08-02T08:24:14.081678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lovasz-Softmax and Jaccard hinge loss in PyTorch\n",
    "Maxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse\n",
    "\n",
    "\n",
    "def lovasz_grad(gt_sorted):\n",
    "    \"\"\"\n",
    "    Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "    See Alg. 1 in paper\n",
    "    \"\"\"\n",
    "    p = len(gt_sorted)\n",
    "    gts = gt_sorted.sum()\n",
    "    intersection = gts - gt_sorted.float().cumsum(0)\n",
    "    union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "    jaccard = 1. - intersection / union\n",
    "    if p > 1: # cover 1-pixel case\n",
    "        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "    return jaccard\n",
    "\n",
    "\n",
    "def iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n",
    "    \"\"\"\n",
    "    IoU for foreground class\n",
    "    binary: 1 foreground, 0 background\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        intersection = ((label == 1) & (pred == 1)).sum()\n",
    "        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n",
    "        if not union:\n",
    "            iou = EMPTY\n",
    "        else:\n",
    "            iou = float(intersection) / union\n",
    "        ious.append(iou)\n",
    "    iou = f_mean(ious)    # mean accross images if per_image\n",
    "    return 100 * iou\n",
    "\n",
    "\n",
    "def iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n",
    "    \"\"\"\n",
    "    Array of IoU for each (non ignored) class\n",
    "    \"\"\"\n",
    "    if not per_image:\n",
    "        preds, labels = (preds,), (labels,)\n",
    "    ious = []\n",
    "    for pred, label in zip(preds, labels):\n",
    "        iou = []    \n",
    "        for i in range(C):\n",
    "            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n",
    "                intersection = ((label == i) & (pred == i)).sum()\n",
    "                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n",
    "                if not union:\n",
    "                    iou.append(EMPTY)\n",
    "                else:\n",
    "                    iou.append(float(intersection) / union)\n",
    "        ious.append(iou)\n",
    "    ious = map(f_mean, zip(*ious)) # mean accross images if per_image\n",
    "    return 100 * np.array(ious)\n",
    "\n",
    "\n",
    "# --------------------------- BINARY LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = f_mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                          for log, lab in zip(logits, labels))\n",
    "    else:\n",
    "        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_hinge_flat(logits, labels):\n",
    "    \"\"\"\n",
    "    Binary Lovasz hinge loss\n",
    "      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "      labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "      ignore: label to ignore\n",
    "    \"\"\"\n",
    "    if len(labels) == 0:\n",
    "        # only void pixels, the gradients should be 0\n",
    "        return logits.sum() * 0.\n",
    "    signs = 2. * labels.float() - 1.\n",
    "    errors = (1. - logits * Variable(signs))\n",
    "    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "    perm = perm.data\n",
    "    gt_sorted = labels[perm]\n",
    "    grad = lovasz_grad(gt_sorted)\n",
    "    #loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n",
    "    loss = torch.dot(F.elu(errors_sorted)+1, Variable(grad))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def flatten_binary_scores(scores, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch (binary case)\n",
    "    Remove labels equal to 'ignore'\n",
    "    \"\"\"\n",
    "    scores = scores.view(-1)\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return scores, labels\n",
    "    valid = (labels != ignore)\n",
    "    vscores = scores[valid]\n",
    "    vlabels = labels[valid]\n",
    "    return vscores, vlabels\n",
    "\n",
    "\n",
    "class StableBCELoss(torch.nn.modules.Module):\n",
    "    def __init__(self):\n",
    "         super(StableBCELoss, self).__init__()\n",
    "    def forward(self, input, target):\n",
    "         neg_abs = - input.abs()\n",
    "         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n",
    "         return loss.mean()\n",
    "\n",
    "\n",
    "def binary_xloss(logits, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Binary Cross entropy loss\n",
    "      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "      ignore: void class id\n",
    "    \"\"\"\n",
    "    logits, labels = flatten_binary_scores(logits, labels, ignore)\n",
    "    loss = StableBCELoss()(logits, Variable(labels.float()))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# --------------------------- MULTICLASS LOSSES ---------------------------\n",
    "\n",
    "\n",
    "def lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "      per_image: compute the loss per image instead of per batch\n",
    "      ignore: void class labels\n",
    "    \"\"\"\n",
    "    if per_image:\n",
    "        loss = f_mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n",
    "                          for prob, lab in zip(probas, labels))\n",
    "    else:\n",
    "        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probas, labels, only_present=False):\n",
    "    \"\"\"\n",
    "    Multi-class Lovasz-Softmax loss\n",
    "      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n",
    "      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n",
    "      only_present: average only on classes present in ground truth\n",
    "    \"\"\"\n",
    "    C = probas.size(1)\n",
    "    losses = []\n",
    "    for c in range(C):\n",
    "        fg = (labels == c).float() # foreground for class c\n",
    "        if only_present and fg.sum() == 0:\n",
    "            continue\n",
    "        errors = (Variable(fg) - probas[:, c]).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n",
    "        perm = perm.data\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n",
    "    return f_mean(losses)\n",
    "\n",
    "\n",
    "def flatten_probas(probas, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Flattens predictions in the batch\n",
    "    \"\"\"\n",
    "    B, C, H, W = probas.size()\n",
    "    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n",
    "    labels = labels.view(-1)\n",
    "    if ignore is None:\n",
    "        return probas, labels\n",
    "    valid = (labels != ignore)\n",
    "    vprobas = probas[valid.nonzero().squeeze()]\n",
    "    vlabels = labels[valid]\n",
    "    return vprobas, vlabels\n",
    "\n",
    "def xloss(logits, labels, ignore=None):\n",
    "    \"\"\"\n",
    "    Cross entropy loss\n",
    "    \"\"\"\n",
    "    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n",
    "\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS ---------------------------\n",
    "\n",
    "def f_mean(l, ignore_nan=False, empty=0):\n",
    "    \"\"\"\n",
    "    nanmean compatible with generators.\n",
    "    \"\"\"\n",
    "    l = iter(l)\n",
    "    if ignore_nan:\n",
    "        l = ifilterfalse(np.isnan, l)\n",
    "    try:\n",
    "        n = 1\n",
    "        acc = next(l)\n",
    "    except StopIteration:\n",
    "        if empty == 'raise':\n",
    "            raise ValueError('Empty mean')\n",
    "        return empty\n",
    "    for n, v in enumerate(l, 2):\n",
    "        acc += v\n",
    "    if n == 1:\n",
    "        return acc\n",
    "    return acc / n\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import os\n",
    "import gc\n",
    "from tqdm.notebook import tqdm\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61beca89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.506516Z",
     "iopub.status.busy": "2022-08-02T08:24:18.505871Z",
     "iopub.status.idle": "2022-08-02T08:24:18.524160Z",
     "shell.execute_reply": "2022-08-02T08:24:18.522802Z"
    },
    "papermill": {
     "duration": 0.029432,
     "end_time": "2022-08-02T08:24:18.527036",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.497604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bs = 64\n",
    "sz = 256    # the size of tiles\n",
    "reduce = 4  # reduce the original images by 4 times\n",
    "expansion = 32\n",
    "TH = 0.225  # threshold for positive predictions\n",
    "DATA = '../input/hubmap-organ-segmentation/test_images/'\n",
    "MODELS = [f'../input/hubmap-fast-ai-starter-efficientnet/model_{i}.pth' for i in range(4)]\n",
    "RES50MODELS = [f'../input/training-fastai-baseline/model_{i}.pth' for i in range(4)]\n",
    "RES101MODELS =  [f'../input/hubmap-new-03-03/model_seed0_fold{i}_bestscore.pth' for i in range (4)]\n",
    "\n",
    "df_sample = pd.read_csv('../input/hubmap-organ-segmentation/sample_submission.csv')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f54c056",
   "metadata": {
    "papermill": {
     "duration": 0.007106,
     "end_time": "2022-08-02T08:24:18.541362",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.534256",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41a6a4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.557932Z",
     "iopub.status.busy": "2022-08-02T08:24:18.557228Z",
     "iopub.status.idle": "2022-08-02T08:24:18.569175Z",
     "shell.execute_reply": "2022-08-02T08:24:18.568189Z"
    },
    "papermill": {
     "duration": 0.023097,
     "end_time": "2022-08-02T08:24:18.571701",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.548604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# functions to convert encoding to mask and mask to encoding\n",
    "def enc2mask(encs, shape):\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for m,enc in enumerate(encs):\n",
    "        if isinstance(enc,np.float) and np.isnan(enc): continue\n",
    "        s = enc.split()\n",
    "        for i in range(len(s)//2):\n",
    "            start = int(s[2*i]) - 1\n",
    "            length = int(s[2*i+1])\n",
    "            img[start:start+length] = 1 + m\n",
    "    return img.reshape(shape).T\n",
    "\n",
    "def mask2enc(mask, n=1):\n",
    "    pixels = mask.T.flatten()\n",
    "    encs = []\n",
    "    for i in range(1,n+1):\n",
    "        p = (pixels == i).astype(np.int8)\n",
    "        if p.sum() == 0: encs.append(np.nan)\n",
    "        else:\n",
    "            p = np.concatenate([[0], p, [0]])\n",
    "            runs = np.where(p[1:] != p[:-1])[0] + 1\n",
    "            runs[1::2] -= runs[::2]\n",
    "            encs.append(' '.join(str(x) for x in runs))\n",
    "    return encs\n",
    "\n",
    "#https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "#with transposed mask\n",
    "def rle_encode_less_memory(img):\n",
    "    #the image should be transposed\n",
    "    pixels = img.T.flatten()\n",
    "    \n",
    "    # This simplified method requires first and last pixel to be zero\n",
    "    pixels[0] = 0\n",
    "    pixels[-1] = 0\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    runs[1::2] -= runs[::2]\n",
    "    \n",
    "    return ' '.join(str(x) for x in runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6b6ab56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.588050Z",
     "iopub.status.busy": "2022-08-02T08:24:18.587282Z",
     "iopub.status.idle": "2022-08-02T08:24:18.608961Z",
     "shell.execute_reply": "2022-08-02T08:24:18.607970Z"
    },
    "papermill": {
     "duration": 0.032611,
     "end_time": "2022-08-02T08:24:18.611415",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.578804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/thedevastator/hubmap-2022-256x256\n",
    "mean = np.array([0.7720342, 0.74582646, 0.76392896])\n",
    "std = np.array([0.24745085, 0.26182273, 0.25782376])\n",
    "\n",
    "s_th = 40  #saturation blancking threshold\n",
    "p_th = 1000*(sz//256)**2 #threshold for the minimum number of pixels\n",
    "identity = rasterio.Affine(1, 0, 0, 0, 1, 0)\n",
    "\n",
    "def img2tensor(img,dtype:np.dtype=np.float32):\n",
    "    if img.ndim==2 : img = np.expand_dims(img,2)\n",
    "    img = np.transpose(img,(2,0,1))\n",
    "    return torch.from_numpy(img.astype(dtype, copy=False))\n",
    "\n",
    "class HuBMAPDataset(Dataset):\n",
    "    def __init__(self, idx, sz=sz, reduce=reduce,expansion=expansion):\n",
    "        self.data = rasterio.open(os.path.join(DATA,idx+'.tiff'), transform = identity,\n",
    "                                 num_threads='all_cpus')\n",
    "        # some images have issues with their format \n",
    "        # and must be saved correctly before reading with rasterio\n",
    "        if self.data.count != 3:\n",
    "            subdatasets = self.data.subdatasets\n",
    "            self.layers = []\n",
    "            if len(subdatasets) > 0:\n",
    "                for i, subdataset in enumerate(subdatasets, 0):\n",
    "                    self.layers.append(rasterio.open(subdataset))\n",
    "        self.shape = self.data.shape\n",
    "        self.reduce = reduce\n",
    "        self.sz = reduce*sz\n",
    "        self.expansion = reduce*expansion\n",
    "        self.pad0 = (self.sz - self.shape[0]%self.sz)%self.sz\n",
    "        self.pad1 = (self.sz - self.shape[1]%self.sz)%self.sz\n",
    "        self.n0max = (self.shape[0] + self.pad0)//self.sz\n",
    "        self.n1max = (self.shape[1] + self.pad1)//self.sz\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n0max*self.n1max\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # the code below may be a little bit difficult to understand,\n",
    "        # but the thing it does is mapping the original image to\n",
    "        # tiles created with adding padding, as done in\n",
    "        # https://www.kaggle.com/iafoss/256x256-images ,\n",
    "        # and then the tiles are loaded with rasterio\n",
    "        # n0,n1 - are the x and y index of the tile (idx = n0*self.n1max + n1)\n",
    "        n0,n1 = idx//self.n1max, idx%self.n1max\n",
    "        # x0,y0 - are the coordinates of the lower left corner of the tile in the image\n",
    "        # negative numbers correspond to padding (which must not be loaded)\n",
    "        x0,y0 = -self.pad0//2 + n0*self.sz - self.expansion//2, -self.pad1//2 + n1*self.sz - self.expansion//2\n",
    "        # make sure that the region to read is within the image\n",
    "        p00,p01 = max(0,x0), min(x0+self.sz+self.expansion,self.shape[0])\n",
    "        p10,p11 = max(0,y0), min(y0+self.sz+self.expansion,self.shape[1])\n",
    "        img = np.zeros((self.sz+self.expansion,self.sz+self.expansion,3),np.uint8)\n",
    "        # mapping the loade region to the tile\n",
    "        if self.data.count == 3:\n",
    "            img[(p00-x0):(p01-x0),(p10-y0):(p11-y0)] = np.moveaxis(self.data.read([1,2,3],\n",
    "                window=Window.from_slices((p00,p01),(p10,p11))), 0, -1)\n",
    "        else:\n",
    "            for i,layer in enumerate(self.layers):\n",
    "                img[(p00-x0):(p01-x0),(p10-y0):(p11-y0),i] =\\\n",
    "                  layer.read(1,window=Window.from_slices((p00,p01),(p10,p11)))\n",
    "        \n",
    "        if self.reduce != 1:\n",
    "            img = cv2.resize(img,((self.sz+self.expansion)//reduce,(self.sz+self.expansion)//reduce),\n",
    "                             interpolation = cv2.INTER_AREA)\n",
    "        #check for empty imges\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "        h,s,v = cv2.split(hsv)\n",
    "        if (s>s_th).sum() <= p_th or img.sum() <= p_th:\n",
    "            #images with -1 will be skipped\n",
    "            return img2tensor((img/255.0 - mean)/std), -1\n",
    "        else: return img2tensor((img/255.0 - mean)/std), idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8daba761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.627367Z",
     "iopub.status.busy": "2022-08-02T08:24:18.626943Z",
     "iopub.status.idle": "2022-08-02T08:24:18.639729Z",
     "shell.execute_reply": "2022-08-02T08:24:18.638789Z"
    },
    "papermill": {
     "duration": 0.023644,
     "end_time": "2022-08-02T08:24:18.642122",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.618478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#iterator like wrapper that returns predicted masks\n",
    "class Model_pred:\n",
    "    def __init__(self, models, dl, tta:bool=True, half:bool=False):\n",
    "        self.models = models\n",
    "        self.dl = dl\n",
    "        self.tta = tta\n",
    "        self.half = half\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count=0\n",
    "        with torch.no_grad():\n",
    "            for x,y in iter(self.dl):\n",
    "                if ((y>=0).sum() > 0): #exclude empty images\n",
    "                    x = x[y>=0].to(device)\n",
    "                    y = y[y>=0]\n",
    "                    if self.half: x = x.half()\n",
    "                    py = None\n",
    "                    for model in self.models:\n",
    "                        p = model(x)\n",
    "                        p = torch.sigmoid(p).detach()\n",
    "                        if py is None: py = p\n",
    "                        else: py += p\n",
    "                    if self.tta:\n",
    "                        #x,y,xy flips as TTA\n",
    "                        flips = [[-1],[-2],[-2,-1]]\n",
    "                        for f in flips:\n",
    "                            xf = torch.flip(x,f)\n",
    "                            for model in self.models:\n",
    "                                p = model(xf)\n",
    "                                p = torch.flip(p,f)\n",
    "                                py += torch.sigmoid(p).detach()\n",
    "                        py /= (1+len(flips))        \n",
    "                    py /= len(self.models)\n",
    "\n",
    "                    py = F.upsample(py, scale_factor=reduce, mode=\"bilinear\")\n",
    "                    py = py.permute(0,2,3,1).float().cpu()\n",
    "                    \n",
    "                    batch_size = len(py)\n",
    "                    for i in range(batch_size):\n",
    "                        yield py[i,expansion*reduce//2:-expansion*reduce//2,expansion*reduce//2:-expansion*reduce//2],y[i]\n",
    "                        count += 1\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.dl.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751a3879",
   "metadata": {
    "papermill": {
     "duration": 0.006855,
     "end_time": "2022-08-02T08:24:18.656057",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.649202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81f25b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.672467Z",
     "iopub.status.busy": "2022-08-02T08:24:18.671769Z",
     "iopub.status.idle": "2022-08-02T08:24:18.697897Z",
     "shell.execute_reply": "2022-08-02T08:24:18.696931Z"
    },
    "papermill": {
     "duration": 0.037219,
     "end_time": "2022-08-02T08:24:18.700353",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.663134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    def __init__(self, input_channels:list, output_channels:list):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Sequential(nn.Conv2d(in_ch, out_ch*2, kernel_size=3, padding=1),\n",
    "             nn.ReLU(inplace=True), nn.BatchNorm2d(out_ch*2),\n",
    "             nn.Conv2d(out_ch*2, out_ch, kernel_size=3, padding=1))\n",
    "            for in_ch, out_ch in zip(input_channels, output_channels)])\n",
    "        \n",
    "    def forward(self, xs:list, last_layer):\n",
    "        hcs = [F.interpolate(c(x),scale_factor=2**(len(self.convs)-i),mode='bilinear') \n",
    "               for i,(c,x) in enumerate(zip(self.convs, xs))]\n",
    "        hcs.append(last_layer)\n",
    "        return torch.cat(hcs, dim=1)\n",
    "\n",
    "class UnetBlock(Module):\n",
    "    def __init__(self, up_in_c:int, x_in_c:int, nf:int=None, blur:bool=False,\n",
    "                 self_attention:bool=False, **kwargs):\n",
    "        super().__init__()\n",
    "        self.shuf = PixelShuffle_ICNR(up_in_c, up_in_c//2, blur=blur, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(x_in_c)\n",
    "        ni = up_in_c//2 + x_in_c\n",
    "        nf = nf if nf is not None else max(up_in_c//2,32)\n",
    "        self.conv1 = ConvLayer(ni, nf, norm_type=None, **kwargs)\n",
    "        self.conv2 = ConvLayer(nf, nf, norm_type=None,\n",
    "            xtra=SelfAttention(nf) if self_attention else None, **kwargs)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, up_in:Tensor, left_in:Tensor) -> Tensor:\n",
    "        s = left_in\n",
    "        up_out = self.shuf(up_in)\n",
    "        cat_x = self.relu(torch.cat([up_out, self.bn(s)], dim=1))\n",
    "        return self.conv2(self.conv1(cat_x))\n",
    "        \n",
    "class _ASPPModule(nn.Module):\n",
    "    def __init__(self, inplanes, planes, kernel_size, padding, dilation, groups=1):\n",
    "        super().__init__()\n",
    "        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n",
    "                stride=1, padding=padding, dilation=dilation, bias=False, groups=groups)\n",
    "        self.bn = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.atrous_conv(x)\n",
    "        x = self.bn(x)\n",
    "\n",
    "        return self.relu(x)\n",
    "\n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, inplanes=512, mid_c=256, dilations=[6, 12, 18, 24], out_c=None):\n",
    "        super().__init__()\n",
    "        self.aspps = [_ASPPModule(inplanes, mid_c, 1, padding=0, dilation=1)] + \\\n",
    "            [_ASPPModule(inplanes, mid_c, 3, padding=d, dilation=d,groups=4) for d in dilations]\n",
    "        self.aspps = nn.ModuleList(self.aspps)\n",
    "        self.global_pool = nn.Sequential(nn.AdaptiveMaxPool2d((1, 1)),\n",
    "                        nn.Conv2d(inplanes, mid_c, 1, stride=1, bias=False),\n",
    "                        nn.BatchNorm2d(mid_c), nn.ReLU())\n",
    "        out_c = out_c if out_c is not None else mid_c\n",
    "        self.out_conv = nn.Sequential(nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False),\n",
    "                                    nn.BatchNorm2d(out_c), nn.ReLU(inplace=True))\n",
    "        self.conv1 = nn.Conv2d(mid_c*(2+len(dilations)), out_c, 1, bias=False)\n",
    "        self._init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.global_pool(x)\n",
    "        xs = [aspp(x) for aspp in self.aspps]\n",
    "        x0 = F.interpolate(x0, size=xs[0].size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat([x0] + xs, dim=1)\n",
    "        return self.out_conv(x)\n",
    "    \n",
    "    def _init_weight(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d42db",
   "metadata": {
    "papermill": {
     "duration": 0.006956,
     "end_time": "2022-08-02T08:24:18.714321",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.707365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61206c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.730536Z",
     "iopub.status.busy": "2022-08-02T08:24:18.729650Z",
     "iopub.status.idle": "2022-08-02T08:24:18.740022Z",
     "shell.execute_reply": "2022-08-02T08:24:18.739065Z"
    },
    "papermill": {
     "duration": 0.021204,
     "end_time": "2022-08-02T08:24:18.742458",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.721254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_root = '../input/efficientnet-pytorch/'\n",
    "efficient_net_encoders = {\n",
    "    \"efficientnet-b0\": {\n",
    "        \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "        \"stage_idxs\": (3, 5, 9, 16),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b0-08094119.pth\"\n",
    "    },\n",
    "    \"efficientnet-b1\": {\n",
    "        \"out_channels\": (3, 32, 24, 40, 112, 320),\n",
    "        \"stage_idxs\": (5, 8, 16, 23),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b1-dbc7070a.pth\"\n",
    "    },\n",
    "    \"efficientnet-b2\": {\n",
    "        \"out_channels\": (3, 32, 24, 48, 120, 352),\n",
    "        \"stage_idxs\": (5, 8, 16, 23),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b2-27687264.pth\"\n",
    "    },\n",
    "    \"efficientnet-b3\": {\n",
    "        \"out_channels\": (3, 40, 32, 48, 136, 384),\n",
    "        \"stage_idxs\": (5, 8, 18, 26),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b3-c8376fa2.pth\"\n",
    "    },\n",
    "    \"efficientnet-b4\": {\n",
    "        \"out_channels\": (3, 48, 32, 56, 160, 448),\n",
    "        \"stage_idxs\": (6, 10, 22, 32),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b4-e116e8b3.pth\"\n",
    "    },\n",
    "    \"efficientnet-b5\": {\n",
    "        \"out_channels\": (3, 48, 40, 64, 176, 512),\n",
    "        \"stage_idxs\": (8, 13, 27, 39),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b5-586e6cc6.pth\"\n",
    "    },\n",
    "    \"efficientnet-b6\": {\n",
    "        \"out_channels\": (3, 56, 40, 72, 200, 576),\n",
    "        \"stage_idxs\": (9, 15, 31, 45),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b6-c76e70fd.pth\"\n",
    "    },\n",
    "    \"efficientnet-b7\": {\n",
    "        \"out_channels\": (3, 64, 48, 80, 224, 640),\n",
    "        \"stage_idxs\": (11, 18, 38, 55),\n",
    "        \"weight_path\": pretrained_root + \"efficientnet-b7-dcc49843.pth\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f5d44e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.759181Z",
     "iopub.status.busy": "2022-08-02T08:24:18.758461Z",
     "iopub.status.idle": "2022-08-02T08:24:18.817378Z",
     "shell.execute_reply": "2022-08-02T08:24:18.816204Z"
    },
    "papermill": {
     "duration": 0.070428,
     "end_time": "2022-08-02T08:24:18.820095",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.749667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../input/efficientnet-pytorch/EfficientNet-PyTorch/EfficientNet-PyTorch-master')\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from efficientnet_pytorch.utils import get_model_params\n",
    "\n",
    "\n",
    "class EfficientNetEncoder(EfficientNet):\n",
    "    def __init__(self, stage_idxs, out_channels, model_name, depth=5):\n",
    "\n",
    "        blocks_args, global_params = get_model_params(model_name, override_params=None)\n",
    "        super().__init__(blocks_args, global_params)\n",
    "        \n",
    "        cfg = efficient_net_encoders[model_name]\n",
    "\n",
    "        self._stage_idxs = stage_idxs\n",
    "        self._out_channels = out_channels\n",
    "        self._depth = depth\n",
    "        self._in_channels = 3\n",
    "\n",
    "        del self._fc\n",
    "        self.load_state_dict(torch.load(cfg['weight_path']))\n",
    "\n",
    "    def get_stages(self):\n",
    "        return [\n",
    "            nn.Identity(),\n",
    "            nn.Sequential(self._conv_stem, self._bn0, self._swish),\n",
    "            self._blocks[:self._stage_idxs[0]],\n",
    "            self._blocks[self._stage_idxs[0]:self._stage_idxs[1]],\n",
    "            self._blocks[self._stage_idxs[1]:self._stage_idxs[2]],\n",
    "            self._blocks[self._stage_idxs[2]:],\n",
    "        ]\n",
    "\n",
    "    def forward(self, x):\n",
    "        stages = self.get_stages()\n",
    "\n",
    "        block_number = 0.\n",
    "        drop_connect_rate = self._global_params.drop_connect_rate\n",
    "\n",
    "        features = []\n",
    "        for i in range(self._depth + 1):\n",
    "\n",
    "            # Identity and Sequential stages\n",
    "            if i < 2:\n",
    "                x = stages[i](x)\n",
    "\n",
    "            # Block stages need drop_connect rate\n",
    "            else:\n",
    "                for module in stages[i]:\n",
    "                    drop_connect = drop_connect_rate * block_number / len(self._blocks)\n",
    "                    block_number += 1.\n",
    "                    x = module(x, drop_connect)\n",
    "\n",
    "            features.append(x)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def load_state_dict(self, state_dict, **kwargs):\n",
    "        state_dict.pop(\"_fc.bias\")\n",
    "        state_dict.pop(\"_fc.weight\")\n",
    "        super().load_state_dict(state_dict, **kwargs)  \n",
    "        \n",
    "\n",
    "class EffUnet(nn.Module):\n",
    "    def __init__(self, model_name, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        cfg = efficient_net_encoders[model_name]\n",
    "        stage_idxs = cfg['stage_idxs']\n",
    "        out_channels = cfg['out_channels']\n",
    "        \n",
    "        self.encoder = EfficientNetEncoder(stage_idxs, out_channels, model_name)\n",
    "\n",
    "        #aspp with customized dilatations\n",
    "        self.aspp = ASPP(out_channels[-1], 256, out_c=384, \n",
    "                         dilations=[stride*1, stride*2, stride*3, stride*4])\n",
    "        self.drop_aspp = nn.Dropout2d(0.5)\n",
    "        #decoder\n",
    "        self.dec4 = UnetBlock(384, out_channels[-2], 256)\n",
    "        self.dec3 = UnetBlock(256, out_channels[-3], 128)\n",
    "        self.dec2 = UnetBlock(128, out_channels[-4], 64)\n",
    "        self.dec1 = UnetBlock(64, out_channels[-5], 32)\n",
    "        self.fpn = FPN([384, 256, 128, 64], [16]*4)\n",
    "        self.drop = nn.Dropout2d(0.1)\n",
    "        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc0, enc1, enc2, enc3, enc4 = self.encoder(x)[-5:]\n",
    "        enc5 = self.aspp(enc4)\n",
    "        dec3 = self.dec4(self.drop_aspp(enc5), enc3)\n",
    "        dec2 = self.dec3(dec3,enc2)\n",
    "        dec1 = self.dec2(dec2,enc1)\n",
    "        dec0 = self.dec1(dec1,enc0)\n",
    "        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n",
    "        return x\n",
    "    \n",
    "#split the model to encoder and decoder for fast.ai\n",
    "split_layers = lambda m: [\n",
    "                list(m.encoder.parameters()),\n",
    "                list(m.aspp.parameters())+list(m.dec4.parameters())+\n",
    "                list(m.dec3.parameters())+list(m.dec2.parameters())+\n",
    "                list(m.dec1.parameters())+list(m.fpn.parameters())+\n",
    "                list(m.final_conv.parameters())\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382225df",
   "metadata": {
    "papermill": {
     "duration": 0.006682,
     "end_time": "2022-08-02T08:24:18.833861",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.827179",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "UNeXt50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ad7424",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.850450Z",
     "iopub.status.busy": "2022-08-02T08:24:18.849739Z",
     "iopub.status.idle": "2022-08-02T08:24:18.863084Z",
     "shell.execute_reply": "2022-08-02T08:24:18.862203Z"
    },
    "papermill": {
     "duration": 0.024157,
     "end_time": "2022-08-02T08:24:18.865476",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.841319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "class UneXt50(nn.Module):\n",
    "    def __init__(self, stride=1, **kwargs):\n",
    "        super().__init__()\n",
    "        #encoder\n",
    "        m = ResNet(Bottleneck, [3, 4, 6, 3], groups=32, width_per_group=4)\n",
    "        #m = torch.hub.load('facebookresearch/semi-supervised-ImageNet1K-models',\n",
    "        #                   'resnext50_32x4d_ssl')\n",
    "        self.enc0 = nn.Sequential(m.conv1, m.bn1, nn.ReLU(inplace=True))\n",
    "        self.enc1 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1),\n",
    "                            m.layer1) #256\n",
    "        self.enc2 = m.layer2 #512\n",
    "        self.enc3 = m.layer3 #1024\n",
    "        self.enc4 = m.layer4 #2048\n",
    "        #aspp with customized dilatations\n",
    "        self.aspp = ASPP(2048,256,out_c=512,dilations=[stride*1,stride*2,stride*3,stride*4])\n",
    "        self.drop_aspp = nn.Dropout2d(0.5)\n",
    "        #decoder\n",
    "        self.dec4 = UnetBlock(512,1024,256)\n",
    "        self.dec3 = UnetBlock(256,512,128)\n",
    "        self.dec2 = UnetBlock(128,256,64)\n",
    "        self.dec1 = UnetBlock(64,64,32)\n",
    "        self.fpn = FPN([512,256,128,64],[16]*4)\n",
    "        self.drop = nn.Dropout2d(0.1)\n",
    "        self.final_conv = ConvLayer(32+16*4, 1, ks=1, norm_type=None, act_cls=None)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc0 = self.enc0(x)\n",
    "        enc1 = self.enc1(enc0)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.aspp(enc4)\n",
    "        dec3 = self.dec4(self.drop_aspp(enc5),enc3)\n",
    "        dec2 = self.dec3(dec3,enc2)\n",
    "        dec1 = self.dec2(dec2,enc1)\n",
    "        dec0 = self.dec1(dec1,enc0)\n",
    "        x = self.fpn([enc5, dec3, dec2, dec1], dec0)\n",
    "        x = self.final_conv(self.drop(x))\n",
    "        x = F.interpolate(x,scale_factor=2,mode='bilinear')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58732d5d",
   "metadata": {
    "papermill": {
     "duration": 0.006699,
     "end_time": "2022-08-02T08:24:18.879341",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.872642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "U-Net_ResNeXt101 + CBAM + hypercolumns + deepsupervision\n",
    "\n",
    "U-Net ResNeXt101 based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21e4bb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.895582Z",
     "iopub.status.busy": "2022-08-02T08:24:18.895101Z",
     "iopub.status.idle": "2022-08-02T08:24:18.901212Z",
     "shell.execute_reply": "2022-08-02T08:24:18.900372Z"
    },
    "papermill": {
     "duration": 0.016522,
     "end_time": "2022-08-02T08:24:18.903162",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.886640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'split_seed_list':[0],\n",
    "    'FOLD_LIST':[0,1,2,3], \n",
    "    'model_path':'../input/hubmap-new-03-03/',\n",
    "    'model_name':'seresnext101_ctrans',\n",
    "    \n",
    "    'num_classes':1,\n",
    "    'resolution':1024, #(1024,1024),(512,512),\n",
    "    'input_resolution':320, #(320,320), #(256,256), #(512,512), #(384,384)\n",
    "    'deepsupervision':False, # always false for inference\n",
    "    'clfhead':False,\n",
    "    'clf_threshold':0.5,\n",
    "    'small_mask_threshold':0, #256*256*0.03, #512*512*0.03,\n",
    "    'mask_threshold':0.5,\n",
    "    'pad_size':256, #(64,64), #(256,256), #(128,128)\n",
    "    \n",
    "    'tta':3,\n",
    "    'test_batch_size':12,\n",
    "    \n",
    "    'FP16':False,\n",
    "    'num_workers':4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b79b851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:18.919771Z",
     "iopub.status.busy": "2022-08-02T08:24:18.919059Z",
     "iopub.status.idle": "2022-08-02T08:24:20.488852Z",
     "shell.execute_reply": "2022-08-02T08:24:20.487726Z"
    },
    "papermill": {
     "duration": 1.581637,
     "end_time": "2022-08-02T08:24:20.491728",
     "exception": false,
     "start_time": "2022-08-02T08:24:18.910091",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "package_dir = \"../input/pretrainedmodels/pretrained-models.pytorch-master/\"\n",
    "sys.path.insert(0, package_dir)\n",
    "import pretrainedmodels\n",
    "\n",
    "def conv3x3(in_channel, out_channel): #not change resolusion\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n",
    "\n",
    "def conv1x1(in_channel, out_channel): #not change resolution\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n",
    "\n",
    "def init_weight(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        #nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "        #nn.init.xavier_normal_(m.weight, gain=1)\n",
    "        #nn.init.kaiming_uniform_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        #nn.init.orthogonal_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('Batch') != -1:\n",
    "        m.weight.data.normal_(1,0.02)\n",
    "        m.bias.data.zero_()\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.orthogonal_(m.weight, gain=1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif classname.find('Embedding') != -1:\n",
    "        nn.init.orthogonal_(m.weight, gain=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c21c05e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:20.509521Z",
     "iopub.status.busy": "2022-08-02T08:24:20.508530Z",
     "iopub.status.idle": "2022-08-02T08:24:20.534770Z",
     "shell.execute_reply": "2022-08-02T08:24:20.533478Z"
    },
    "papermill": {
     "duration": 0.037404,
     "end_time": "2022-08-02T08:24:20.537654",
     "exception": false,
     "start_time": "2022-08-02T08:24:20.500250",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.theta    = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n",
    "        self.phi      = nn.utils.spectral_norm(conv1x1(channels, channels//8)).apply(init_weight)\n",
    "        self.g        = nn.utils.spectral_norm(conv1x1(channels, channels//2)).apply(init_weight)\n",
    "        self.o        = nn.utils.spectral_norm(conv1x1(channels//2, channels)).apply(init_weight)\n",
    "        self.gamma    = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        batch,c,h,w = inputs.size()\n",
    "        theta = self.theta(inputs) #->(*,c/8,h,w)\n",
    "        phi   = F.max_pool2d(self.phi(inputs), [2,2]) #->(*,c/8,h/2,w/2)\n",
    "        g     = F.max_pool2d(self.g(inputs), [2,2]) #->(*,c/2,h/2,w/2)\n",
    "        \n",
    "        theta = theta.view(batch, self.channels//8, -1) #->(*,c/8,h*w)\n",
    "        phi   = phi.view(batch, self.channels//8, -1) #->(*,c/8,h*w/4)\n",
    "        g     = g.view(batch, self.channels//2, -1) #->(*,c/2,h*w/4)\n",
    "        \n",
    "        beta = F.softmax(torch.bmm(theta.transpose(1,2), phi), -1) #->(*,h*w,h*w/4)\n",
    "        o    = self.o(torch.bmm(g, beta.transpose(1,2)).view(batch,self.channels//2,h,w)) #->(*,c,h,w)\n",
    "        return self.gamma*o + inputs\n",
    "    \n",
    "    \n",
    "\n",
    "class ChannelAttentionModule(nn.Module):\n",
    "    def __init__(self, in_channel, reduction):\n",
    "        super().__init__()\n",
    "        self.global_maxpool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.global_avgpool = nn.AdaptiveAvgPool2d(1) \n",
    "        self.fc = nn.Sequential(\n",
    "            conv1x1(in_channel, in_channel//reduction).apply(init_weight),\n",
    "            nn.ReLU(True),\n",
    "            conv1x1(in_channel//reduction, in_channel).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1 = self.global_maxpool(inputs)\n",
    "        x2 = self.global_avgpool(inputs)\n",
    "        x1 = self.fc(x1)\n",
    "        x2 = self.fc(x2)\n",
    "        x  = torch.sigmoid(x1 + x2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SpatialAttentionModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv3x3 = conv3x3(2,1).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x1,_ = torch.max(inputs, dim=1, keepdim=True)\n",
    "        x2 = torch.mean(inputs, dim=1, keepdim=True)\n",
    "        x  = torch.cat([x1,x2], dim=1)\n",
    "        x  = self.conv3x3(x)\n",
    "        x  = torch.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_channel, reduction):\n",
    "        super().__init__()\n",
    "        self.channel_attention = ChannelAttentionModule(in_channel, reduction)\n",
    "        self.spatial_attention = SpatialAttentionModule()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = inputs * self.channel_attention(inputs)\n",
    "        x = x * self.spatial_attention(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class CenterBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_channel, out_channel).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.conv(inputs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecodeBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, upsample):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channel).apply(init_weight)\n",
    "        self.upsample = nn.Sequential()\n",
    "        if upsample:\n",
    "            self.upsample.add_module('upsample',nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "        self.conv3x3_1 = conv3x3(in_channel, in_channel).apply(init_weight)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channel).apply(init_weight)\n",
    "        self.conv3x3_2 = conv3x3(in_channel, out_channel).apply(init_weight)\n",
    "        self.cbam = CBAM(out_channel, reduction=16)\n",
    "        self.conv1x1   = conv1x1(in_channel, out_channel).apply(init_weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x  = F.relu(self.bn1(inputs))\n",
    "        x  = self.upsample(x)\n",
    "        x  = self.conv3x3_1(x)\n",
    "        x  = self.conv3x3_2(F.relu(self.bn2(x)))\n",
    "        x  = self.cbam(x)\n",
    "        x += self.conv1x1(self.upsample(inputs)) #shortcut\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e6218bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:20.554343Z",
     "iopub.status.busy": "2022-08-02T08:24:20.553865Z",
     "iopub.status.idle": "2022-08-02T08:24:20.582031Z",
     "shell.execute_reply": "2022-08-02T08:24:20.581172Z"
    },
    "papermill": {
     "duration": 0.039647,
     "end_time": "2022-08-02T08:24:20.584509",
     "exception": false,
     "start_time": "2022-08-02T08:24:20.544862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UNET_SERESNEXT101(nn.Module):\n",
    "    def __init__(self, resolution, deepsupervision=False, clfhead=False, load_weights=False):\n",
    "        super().__init__()\n",
    "        h,w = resolution\n",
    "        self.deepsupervision = deepsupervision\n",
    "        self.clfhead = clfhead\n",
    "        \n",
    "        #encoder\n",
    "        model_name = 'se_resnext101_32x4d'\n",
    "        seresnext101 = pretrainedmodels.__dict__[model_name](pretrained=None)\n",
    "        if load_weights:\n",
    "            seresnext101.load_state_dict(torch.load(f'../../../pretrainedmodels_weight/{model_name}.pth'))\n",
    "        \n",
    "        self.encoder0 = nn.Sequential(\n",
    "            seresnext101.layer0.conv1, #(*,3,h,w)->(*,64,h/2,w/2)\n",
    "            seresnext101.layer0.bn1,\n",
    "            seresnext101.layer0.relu1,\n",
    "        )\n",
    "        self.encoder1 = nn.Sequential(\n",
    "            seresnext101.layer0.pool, #->(*,64,h/4,w/4)\n",
    "            seresnext101.layer1 #->(*,256,h/4,w/4)\n",
    "        )\n",
    "        self.encoder2 = seresnext101.layer2 #->(*,512,h/8,w/8)\n",
    "        self.encoder3 = seresnext101.layer3 #->(*,1024,h/16,w/16)\n",
    "        self.encoder4 = seresnext101.layer4 #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #center\n",
    "        self.center  = CenterBlock(2048,512) #->(*,512,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        self.decoder4 = DecodeBlock(512+2048,64, upsample=True) #->(*,64,h/16,w/16)\n",
    "        self.decoder3 = DecodeBlock(64+1024,64, upsample=True) #->(*,64,h/8,w/8)\n",
    "        self.decoder2 = DecodeBlock(64+512,64,  upsample=True) #->(*,64,h/4,w/4) \n",
    "        self.decoder1 = DecodeBlock(64+256,64,   upsample=True) #->(*,64,h/2,w/2) \n",
    "        self.decoder0 = DecodeBlock(64,64, upsample=True) #->(*,64,h,w) \n",
    "        \n",
    "        #upsample\n",
    "        self.upsample4 = nn.Upsample(scale_factor=16, mode='bilinear', align_corners=True)\n",
    "        self.upsample3 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        #deep supervision\n",
    "        self.deep4 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep3 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep2 = conv1x1(64,1).apply(init_weight)\n",
    "        self.deep1 = conv1x1(64,1).apply(init_weight)\n",
    "        \n",
    "        #final conv\n",
    "        self.final_conv = nn.Sequential(\n",
    "            conv3x3(320,64).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            conv1x1(64,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "        #clf head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.BatchNorm1d(2048).apply(init_weight),\n",
    "            nn.Linear(2048,512).apply(init_weight),\n",
    "            nn.ELU(True),\n",
    "            nn.BatchNorm1d(512).apply(init_weight),\n",
    "            nn.Linear(512,1).apply(init_weight)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        #encoder\n",
    "        x0 = self.encoder0(inputs) #->(*,64,h/2,w/2)\n",
    "        x1 = self.encoder1(x0) #->(*,256,h/4,w/4)\n",
    "        x2 = self.encoder2(x1) #->(*,512,h/8,w/8)\n",
    "        x3 = self.encoder3(x2) #->(*,1024,h/16,w/16)\n",
    "        x4 = self.encoder4(x3) #->(*,2048,h/32,w/32)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        if config['clf_threshold'] is not None:\n",
    "            if (torch.sigmoid(logits_clf)>config['clf_threshold']).sum().item()==0:\n",
    "                bs,_,h,w = inputs.shape\n",
    "                logits = torch.zeros((bs,1,h,w))\n",
    "                if self.clfhead:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_,_\n",
    "                    else:\n",
    "                        return logits,_\n",
    "                else:\n",
    "                    if self.deepsupervision:\n",
    "                        return logits,_\n",
    "                    else:\n",
    "                        return logits\n",
    "        \n",
    "        #center\n",
    "        y5 = self.center(x4) #->(*,320,h/32,w/32)\n",
    "        \n",
    "        #decoder\n",
    "        y4 = self.decoder4(torch.cat([x4,y5], dim=1)) #->(*,64,h/16,w/16)\n",
    "        y3 = self.decoder3(torch.cat([x3,y4], dim=1)) #->(*,64,h/8,w/8)\n",
    "        y2 = self.decoder2(torch.cat([x2,y3], dim=1)) #->(*,64,h/4,w/4)\n",
    "        y1 = self.decoder1(torch.cat([x1,y2], dim=1)) #->(*,64,h/2,w/2) \n",
    "        y0 = self.decoder0(y1) #->(*,64,h,w)\n",
    "        \n",
    "        #hypercolumns\n",
    "        y4 = self.upsample4(y4) #->(*,64,h,w)\n",
    "        y3 = self.upsample3(y3) #->(*,64,h,w)\n",
    "        y2 = self.upsample2(y2) #->(*,64,h,w)\n",
    "        y1 = self.upsample1(y1) #->(*,64,h,w)\n",
    "        hypercol = torch.cat([y0,y1,y2,y3,y4], dim=1)\n",
    "        \n",
    "        #final conv\n",
    "        logits = self.final_conv(hypercol) #->(*,1,h,w)\n",
    "        \n",
    "        #clf head\n",
    "        logits_clf = self.clf(self.avgpool(x4).squeeze(-1).squeeze(-1)) #->(*,1)\n",
    "        \n",
    "        if self.clfhead:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps, logits_clf\n",
    "            else:\n",
    "                return logits, logits_clf\n",
    "        else:\n",
    "            if self.deepsupervision:\n",
    "                s4 = self.deep4(y4)\n",
    "                s3 = self.deep3(y3)\n",
    "                s2 = self.deep2(y2)\n",
    "                s1 = self.deep1(y1)\n",
    "                logits_deeps = [s4,s3,s2,s1]\n",
    "                return logits, logits_deeps\n",
    "            else:\n",
    "                return logits    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d36598ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:24:20.600985Z",
     "iopub.status.busy": "2022-08-02T08:24:20.599792Z",
     "iopub.status.idle": "2022-08-02T08:25:23.879194Z",
     "shell.execute_reply": "2022-08-02T08:25:23.877435Z"
    },
    "papermill": {
     "duration": 63.293189,
     "end_time": "2022-08-02T08:25:23.884864",
     "exception": false,
     "start_time": "2022-08-02T08:24:20.591675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    state_dict = torch.load(path,map_location=torch.device('cpu'))\n",
    "    model1 = EffUnet('efficientnet-b5')\n",
    "    model1.load_state_dict(state_dict)\n",
    "    model1.float()\n",
    "    model1.eval()\n",
    "    model1.to(device)\n",
    "    models.append(model1)\n",
    "    \n",
    "del state_dict\n",
    "\n",
    "\n",
    "for path in RES50MODELS:\n",
    "    state_dict = torch.load(path,map_location=torch.device('cpu'))\n",
    "    model2 = UneXt50()\n",
    "    model2.load_state_dict(state_dict)\n",
    "    model2.float()\n",
    "    model2.eval()\n",
    "    model2.to(device)\n",
    "    models.append(model2)\n",
    "\n",
    "del state_dict\n",
    "\n",
    "\n",
    "for path in RES101MODELS:\n",
    "    state_dict = torch.load(path,map_location=torch.device('cpu'))\n",
    "    model3 = UNET_SERESNEXT101(resolution=(None, None))\n",
    "    model3.load_state_dict(state_dict)\n",
    "    model3.float()\n",
    "    model3.eval()\n",
    "    model3.to(device)\n",
    "    models.append(model3)\n",
    "\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fe3c5",
   "metadata": {
    "papermill": {
     "duration": 0.006914,
     "end_time": "2022-08-02T08:25:23.901496",
     "exception": false,
     "start_time": "2022-08-02T08:25:23.894582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "668afaf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:25:23.917953Z",
     "iopub.status.busy": "2022-08-02T08:25:23.917518Z",
     "iopub.status.idle": "2022-08-02T08:26:52.574282Z",
     "shell.execute_reply": "2022-08-02T08:26:52.573121Z"
    },
    "papermill": {
     "duration": 88.676344,
     "end_time": "2022-08-02T08:26:52.585078",
     "exception": false,
     "start_time": "2022-08-02T08:25:23.908734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aff4750f71a497fac0629cdc2959ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "names,preds = [],[]\n",
    "for idx,row in tqdm(df_sample.iterrows(),total=len(df_sample)):\n",
    "    idx = str(row['id'])\n",
    "    ds = HuBMAPDataset(idx)\n",
    "    #rasterio cannot be used with multiple workers\n",
    "    dl = DataLoader(ds,bs,num_workers=0,shuffle=False,pin_memory=True)\n",
    "    mp = Model_pred(models,dl)\n",
    "    #generate masks\n",
    "    mask = torch.zeros(len(ds),ds.sz,ds.sz,dtype=torch.int8)\n",
    "    for p,i in iter(mp): mask[i.item()] = p.squeeze(-1) > TH\n",
    "    \n",
    "    #reshape tiled masks into a single mask and crop padding\n",
    "    mask = mask.view(ds.n0max,ds.n1max,ds.sz,ds.sz).\\\n",
    "        permute(0,2,1,3).reshape(ds.n0max*ds.sz,ds.n1max*ds.sz)\n",
    "    mask = mask[ds.pad0//2:-(ds.pad0-ds.pad0//2) if ds.pad0 > 0 else ds.n0max*ds.sz,\n",
    "        ds.pad1//2:-(ds.pad1-ds.pad1//2) if ds.pad1 > 0 else ds.n1max*ds.sz]\n",
    "    \n",
    "    #convert to rle\n",
    "    #https://www.kaggle.com/bguberfain/memory-aware-rle-encoding\n",
    "    rle = rle_encode_less_memory(mask.numpy())\n",
    "    names.append(idx)\n",
    "    preds.append(rle)\n",
    "    del mask, ds, dl\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b1616e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:26:52.601391Z",
     "iopub.status.busy": "2022-08-02T08:26:52.600902Z",
     "iopub.status.idle": "2022-08-02T08:26:52.612680Z",
     "shell.execute_reply": "2022-08-02T08:26:52.611614Z"
    },
    "papermill": {
     "duration": 0.022519,
     "end_time": "2022-08-02T08:26:52.614893",
     "exception": false,
     "start_time": "2022-08-02T08:26:52.592374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'id':names,'rle':preds})\n",
    "df.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "def29e79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-02T08:26:52.631445Z",
     "iopub.status.busy": "2022-08-02T08:26:52.631003Z",
     "iopub.status.idle": "2022-08-02T08:26:52.650020Z",
     "shell.execute_reply": "2022-08-02T08:26:52.649209Z"
    },
    "papermill": {
     "duration": 0.029997,
     "end_time": "2022-08-02T08:26:52.652130",
     "exception": false,
     "start_time": "2022-08-02T08:26:52.622133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10078</td>\n",
       "      <td>314 224 2285 339 4286 384 6298 397 8313 407 10328 417 12345 424 13150 1 14363 429 15171 3 16382 434 17192 5 18403 436 19213 7 20423 439 21236 7 22446 438 23260 6 24468 438 25285 4 26490 437 27309 3 28513 436 29333 2 30535 436 32558 434 34581 433 36604 432 38627 430 40650 429 42673 427 44697 424 46720 422 48743 420 50767 417 52790 55 52854 351 54814 49 54881 346 56839 42 56906 342 58863 27 58897 2 58931 339 60886 26 60956 336 62910 24 62981 333 64934 22 65006 330 66958 20 67032 326 68983 17 69071 309 71011 10 71097 306 73121 304 75146 301 77172 297 79202 290 81228 286 83254 282 85280 277 87...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  10078   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       rle  \n",
       "0  314 224 2285 339 4286 384 6298 397 8313 407 10328 417 12345 424 13150 1 14363 429 15171 3 16382 434 17192 5 18403 436 19213 7 20423 439 21236 7 22446 438 23260 6 24468 438 25285 4 26490 437 27309 3 28513 436 29333 2 30535 436 32558 434 34581 433 36604 432 38627 430 40650 429 42673 427 44697 424 46720 422 48743 420 50767 417 52790 55 52854 351 54814 49 54881 346 56839 42 56906 342 58863 27 58897 2 58931 339 60886 26 60956 336 62910 24 62981 333 64934 22 65006 330 66958 20 67032 326 68983 17 69071 309 71011 10 71097 306 73121 304 75146 301 77172 297 79202 290 81228 286 83254 282 85280 277 87...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b7b937",
   "metadata": {
    "papermill": {
     "duration": 0.006963,
     "end_time": "2022-08-02T08:26:52.666436",
     "exception": false,
     "start_time": "2022-08-02T08:26:52.659473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 170.370138,
   "end_time": "2022-08-02T08:26:54.300972",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-08-02T08:24:03.930834",
   "version": "2.3.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1aff4750f71a497fac0629cdc2959ff0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80eb413967894552859230b621f246fd",
        "IPY_MODEL_aa449a2d379247ba8ae26ad41811f243",
        "IPY_MODEL_a8bbfa1f43d344e6b99efa9a906dd681"
       ],
       "layout": "IPY_MODEL_7ea6390c865c4bcfb1b2e057ccef602a"
      }
     },
     "7b3a63e8338c4bff93ce9c373bdcbd5a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ea6390c865c4bcfb1b2e057ccef602a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80eb413967894552859230b621f246fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8c04e58039fc4643863475199228b43d",
       "placeholder": "​",
       "style": "IPY_MODEL_8ffa0b1f02ae4eaaabdaa440c6d7a0e7",
       "value": "100%"
      }
     },
     "8c04e58039fc4643863475199228b43d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8c41fa3870844dc893ff5aa295271000": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "8ffa0b1f02ae4eaaabdaa440c6d7a0e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a8bbfa1f43d344e6b99efa9a906dd681": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_df025ce79deb4fffa2252ecf730ab0ea",
       "placeholder": "​",
       "style": "IPY_MODEL_d0db6457e392409c96a1a52c48a17046",
       "value": " 1/1 [01:28&lt;00:00, 88.64s/it]"
      }
     },
     "aa449a2d379247ba8ae26ad41811f243": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7b3a63e8338c4bff93ce9c373bdcbd5a",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8c41fa3870844dc893ff5aa295271000",
       "value": 1.0
      }
     },
     "d0db6457e392409c96a1a52c48a17046": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "df025ce79deb4fffa2252ecf730ab0ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
